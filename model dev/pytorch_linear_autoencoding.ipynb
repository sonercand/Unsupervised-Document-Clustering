{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '..\\data'\n",
    "df = pd.read_parquet(path+'\\\\data_scientist_merged_01_09_2019.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>closing</th>\n",
       "      <th>company</th>\n",
       "      <th>contract_time</th>\n",
       "      <th>contract_type</th>\n",
       "      <th>description</th>\n",
       "      <th>disability_confident</th>\n",
       "      <th>id</th>\n",
       "      <th>isco_code</th>\n",
       "      <th>location</th>\n",
       "      <th>postcode</th>\n",
       "      <th>posted</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_base</th>\n",
       "      <th>salary_info</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-12T15:02:30</td>\n",
       "      <td>Hays Specialist Recruitment</td>\n",
       "      <td>full_time</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Data Scientist - Python, Machine Learning, Clo...</td>\n",
       "      <td>None</td>\n",
       "      <td>2038758</td>\n",
       "      <td>2</td>\n",
       "      <td>Bromley, Kent, BR11DP</td>\n",
       "      <td>BR11DP</td>\n",
       "      <td>2019-04-12T15:02:30</td>\n",
       "      <td>£55,000.00 to £65,000.00 per year</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://findajob.dwp.gov.uk/details/2038758?ut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-10T10:42:14</td>\n",
       "      <td>Technojobs</td>\n",
       "      <td>full_time</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Data Scientist Salary £55,788 to £59,297 (depe...</td>\n",
       "      <td>None</td>\n",
       "      <td>2016089</td>\n",
       "      <td>2</td>\n",
       "      <td>London</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-04-10T10:42:14</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://findajob.dwp.gov.uk/details/2016089?ut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               closing                      company contract_time  \\\n",
       "0  2019-05-12T15:02:30  Hays Specialist Recruitment     full_time   \n",
       "1  2019-05-10T10:42:14                   Technojobs     full_time   \n",
       "\n",
       "  contract_type                                        description  \\\n",
       "0     permanent  Data Scientist - Python, Machine Learning, Clo...   \n",
       "1     permanent  Data Scientist Salary £55,788 to £59,297 (depe...   \n",
       "\n",
       "  disability_confident       id  isco_code               location postcode  \\\n",
       "0                 None  2038758          2  Bromley, Kent, BR11DP   BR11DP   \n",
       "1                 None  2016089          2                 London     None   \n",
       "\n",
       "                posted                             salary salary_base  \\\n",
       "0  2019-04-12T15:02:30  £55,000.00 to £65,000.00 per year        None   \n",
       "1  2019-04-10T10:42:14                                           None   \n",
       "\n",
       "  salary_info           title  \\\n",
       "0        None  Data Scientist   \n",
       "1        None  Data Scientist   \n",
       "\n",
       "                                                 url  \n",
       "0  https://findajob.dwp.gov.uk/details/2038758?ut...  \n",
       "1  https://findajob.dwp.gov.uk/details/2016089?ut...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data Scientists Required - Machine Learning - Bristol - 60k Your new company I am working with an exciting organisation; a leading provider of scientific instrumentation for the measurement of elemental concentrations, crystallographic structure, molecular structure and more. They are looking to bring on a dynamic team of Data Scientists to fill their new Bristol office. Your new role As a Data Scientist you will provide technical expertise in the discovery of information buried within large amounts of data and to build machine learning models trained on example data sets that can solve advanced classification and regression problems. This is to facilitate the delivery of next generation products and services, aligned with the company's vision of moving towards predictive and prescriptive solutions which deliver significant new value to our customers. What you'll need to succeed The successful candidate will have a degree level or equivalent experience in Mathematics or Computer Science field. You will also have experience within a combination of the following; Microsoft Azure and/or other cloud deployment platforms Data Mining ML techniques and algorithms; especially Neural Networks, Decision Forests, SVM, Anomaly Detection Data visualisation and reporting tools Designing and developing technologically advanced products/services Software development(Python or R) Matlab, NumPy, Keras, Scikitlearn Automated Machine learning tools You MUST have experience of Machine Learning techniques / algorithms What you'll get in return You will get receive a generous salary, and an opportunity to work within a new team of dynamic, like minded Data Scientists. This will be within an ambitious organisation, striving to learn new things. What you need to do now If you're interested in this role, click 'apply now' to forward an up-to-date copy of your CV, or call us now. If this job isn't quite right for you but you are looking for a new position, please contact us for a confidential discussion on your career. Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. By applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.uk\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### flatten description field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = ''\n",
    "for key,text in df.description.items():\n",
    "    corpora+='\\n'+text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### remove punct from flattened text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = corpora.lower() # lowercase, standardize\n",
    "corpora = [letter for letter in corpora if letter not in punctuation] \n",
    "all_text = ''.join(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata scientist  python machine learning cloud what is the role this is a data scientist position for a leading insurance company based in bromley theyre looking for their next data scientist to work alongside the head of analytics  data science to help drive machine learning forwards across the wider business you will be working to figure out which algorithms are the most robust and scalable for each situation to ensure that this leading brand are making insurance easier and better value for their customers you will be applying groundbreaking technology to their business problems  always staying up to date with machine learning technologies what do you need to succeed handson python experience building productionised data science pipelines experience using open source machine learning packages and frameworks scikitlearn tensorflow experience using cloud based tech aws gcp experience working across internal or external consulting to solve business challenges experience shaping best pra'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = all_text.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1437775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_dict = Counter(words) # get word counts\n",
    "word_counts = {k: v for k, v in sorted(word_counts_dict.items(), reverse=True,key=lambda item: item[1])} # sort by counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'the', 'to', 'of', 'a', 'in', 'data', 'with', 'for', 'will']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(word_counts.keys())\n",
    "keys[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert words to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word to int dictionary\n",
    "word_to_int = {}\n",
    "int_ = 1\n",
    "for key,value in word_counts.items():\n",
    "    word_to_int[key] = int_\n",
    "    int_+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2934\n",
      "2934\n"
     ]
    }
   ],
   "source": [
    "descriptions = all_text.split('\\n')\n",
    "descriptions = descriptions[1:] # first one is an error\n",
    "print(len(descriptions))\n",
    "print(len(df.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert words  into numbers\n",
    "descs_ints = []\n",
    "for desc in descriptions:\n",
    "    current_int =[]\n",
    "    words = desc.split()\n",
    "    for word in words:\n",
    "        try:\n",
    "            current_int.append(word_to_int[word])\n",
    "        except Exception as e:\n",
    "            #review_int.append(0) # replace the removed words with a 0\n",
    "            print(e)\n",
    "    descs_ints.append(current_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized review: \n",
      " [[7, 175, 96, 60, 39, 234, 81, 17, 2, 37, 25, 17, 5, 7, 175, 183, 9, 5, 132, 743, 69, 110, 6, 10968, 3735, 89, 9, 46, 379, 7, 175, 3, 24, 583, 2, 612, 4, 54, 7, 26, 3, 71, 177, 60, 39, 4542, 48, 2, 411, 29, 11, 10, 14, 32, 3, 6457, 200, 101, 325, 21, 2, 225, 725, 1, 775, 9, 697, 4689, 3, 122, 31, 25, 132, 994, 21, 271, 743, 2777, 1, 308, 218, 9, 46, 171, 11, 10, 14, 393, 2351, 68, 3, 46, 29, 182, 954, 4082, 180, 3, 74, 8, 60, 39, 125, 81, 112, 11, 115, 3, 719, 603, 96, 16, 142, 6132, 7, 26, 461, 16, 61, 420, 628, 60, 39, 713, 1, 568, 1492, 1513, 16, 61, 234, 110, 599, 419, 1933, 16, 32, 48, 248, 19, 264, 811, 3, 474, 29, 363, 16, 1977, 170, 4543, 6, 7, 26, 9, 5, 5000, 4690, 29, 19, 911, 81, 10, 11, 293, 6, 518, 25, 17, 22, 1545, 1014, 3, 130, 5, 132, 1780, 994, 6, 5, 2624, 1035, 373, 4, 2, 29, 11, 10, 14, 34, 2, 383, 4, 5, 23, 45, 2, 29, 79, 134, 72, 5, 1878, 193, 20, 619, 25, 69, 4542, 6, 68, 11, 10, 293, 2, 1014, 3, 24, 583, 93, 143, 51, 30, 288, 57, 33, 776, 32, 9, 22, 2398, 5185, 994, 81, 112, 11, 115, 3, 112, 253, 97, 650, 447, 6, 25, 37, 536, 95, 253, 3, 347, 22, 881, 866, 4, 30, 338, 19, 672, 5385, 5186, 253, 20, 5852, 1514, 458, 360, 564, 1483, 15, 22, 391, 555, 9, 524, 360, 1, 391, 29, 9, 2, 744, 4, 1441, 1645, 40, 393, 9, 25, 119, 11, 1340, 2, 1679, 990, 329, 1, 1680, 101, 64, 14, 1026, 34, 1681]]\n"
     ]
    }
   ],
   "source": [
    "# print tokens in first description\n",
    "print('Tokenized review: \\n', descs_ints[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remove zero lenght ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 1), dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only check for zero lenght reviews at this point\n",
    "desc_lengths = np.array([len(desc) for desc in descs_ints])\n",
    "zero_index = np.argwhere(desc_lengths==0)\n",
    "zero_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490.03919563735514, 492.0, 1804, 54)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(desc_lengths),np.median(desc_lengths),np.max(desc_lengths),np.min(desc_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pad and truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trun(descs, seq_length):\n",
    "    \n",
    "    features = np.zeros((len(descs),seq_length),dtype=int)\n",
    "    for j, desc in enumerate(descs):\n",
    "        features[j,-len(desc):] = desc[:seq_length]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 130   88    6 1357   15    5    7  175 5612   37]\n",
      " [   2   29  130   88    6 1357   15    5    7  175]\n",
      " [   2   29  130   88    6 1357   15    5    7  175]\n",
      " [   2   29  130   88    6 1357   15    5    7  175]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 450\n",
    "\n",
    "features = pad_trun(descs_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(descs_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "# print first 10 values of the first 10 batches \n",
    "print(features[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split train validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_frac = 0.9\n",
    "train_size = int(train_size_frac*len(features))\n",
    "train_x, valid_x = features[:train_size],features[train_size:]\n",
    "train_y = np.ones(train_x.shape[0])\n",
    "valid_y = np.ones(valid_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2640, 450), (294, 450), (294,), (2640,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,valid_x.shape,valid_y.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x),torch.from_numpy(valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# shuffle sets \n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 450])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x = dataiter.next()\n",
    "print(sample_x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (embedding): Embedding(25262, 100)\n",
      "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=450, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size,embedding_dim,vocab_size,encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        ## encoder ##\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_dim, encoding_dim)\n",
    "        \n",
    "        ## decoder ##\n",
    "        \n",
    "        self.fc2 = nn.Linear(encoding_dim, embedding_dim)\n",
    "        self.fc3 = nn.Linear(embedding_dim,input_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.long()\n",
    "        # add layer, with relu activation function\n",
    "        x = self.embedding(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # output layer (sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "input_dim = 450\n",
    "embedding_dim = 100\n",
    "encoding_dim = 32\n",
    "vocab_size = len(word_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "model = Autoencoder(input_dim,embedding_dim,vocab_size,encoding_dim)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 450, 450])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "sample_x\n",
    "y = model(sample_x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5407, 0.5088, 0.5020,  ..., 0.5761, 0.5368, 0.5071],\n",
       "        [0.5313, 0.5174, 0.5093,  ..., 0.5790, 0.5368, 0.5216],\n",
       "        [0.5229, 0.5091, 0.5044,  ..., 0.5635, 0.5368, 0.5216],\n",
       "        ...,\n",
       "        [0.5287, 0.5149, 0.5059,  ..., 0.5761, 0.5368, 0.5220],\n",
       "        [0.5343, 0.5149, 0.5019,  ..., 0.5761, 0.5368, 0.5216],\n",
       "        [0.5522, 0.5140, 0.5208,  ..., 0.5642, 0.5368, 0.5078]],\n",
       "       grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = torch.max(y,dim=1, keepdim=False)\n",
    "y1[0].shape\n",
    "y1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 308110183.018868\n",
      "Epoch: 2 \tTraining Loss: 308110181.320755\n",
      "Epoch: 3 \tTraining Loss: 308110181.981132\n",
      "Epoch: 4 \tTraining Loss: 308110181.792453\n",
      "Epoch: 5 \tTraining Loss: 308110181.132075\n",
      "Epoch: 6 \tTraining Loss: 308110180.188679\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        descriptions,_ = data\n",
    "        descriptions= descriptions.view(descriptions.size(0), -1)\n",
    "        # clear the gradients of all optimized variables\n",
    "        descriptions = torch.autograd.Variable(descriptions)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(descriptions)\n",
    "        outputs = torch.max(outputs,dim=1)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs[0], descriptions.float())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*descriptions.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
